{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import cv2\n",
    "import cPickle\n",
    "import numpy as np\n",
    "\n",
    "from faster_rcnn import network\n",
    "from faster_rcnn.faster_rcnn import FasterRCNN, RPN\n",
    "from faster_rcnn.utils.timer import Timer\n",
    "from faster_rcnn.fast_rcnn.nms_wrapper import nms\n",
    "\n",
    "from faster_rcnn.fast_rcnn.bbox_transform import bbox_transform_inv, clip_boxes\n",
    "from faster_rcnn.datasets.factory import get_imdb\n",
    "from faster_rcnn.fast_rcnn.config import cfg, cfg_from_file, get_output_dir\n",
    "\n",
    "from faster_rcnn.pose_net import PoseNet\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 300\n",
    "\n",
    "\n",
    "# hyper-parameters\n",
    "# ------------\n",
    "#imdb_name = 'voc_2007_test'\n",
    "#imdb_name = 'kittivoc_train'\n",
    "#imdb_name = 'kittivoc_val'\n",
    "imdb_name = 'kittipose_val'\n",
    "#imdb_name = 'kitti_train'\n",
    "cfg_file = 'experiments/cfgs/faster_rcnn_end2end.yml'\n",
    "trained_model = 'trained_models/saved_pose_model_final/faster_rcnn_80000.h5'\n",
    "\n",
    "# rand_seed = 1024\n",
    "\n",
    "save_name = 'faster_rcnn_100000'\n",
    "max_per_image = 300\n",
    "thresh = 0.05\n",
    "vis = True\n",
    "\n",
    "# ------------\n",
    "\n",
    "# load config\n",
    "cfg_from_file(cfg_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method kittipose.default_roidb of <faster_rcnn.datasets.kittipose.kittipose object at 0x7f7d3e8eab90>>\n",
      "Remove empty annotations:  Done. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pculbert/.virtualenvs/temp/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load model successfully!\n"
     ]
    }
   ],
   "source": [
    "# load data\n",
    "imdb = get_imdb(imdb_name)\n",
    "imdb.competition_mode(on=True)\n",
    "\n",
    "# load net\n",
    "net = PoseNet(classes=imdb.classes, debug=False)\n",
    "network.load_net(trained_model, net)\n",
    "print('load model successfully!')\n",
    "\n",
    "net.cuda()\n",
    "net.eval(); # set network to evaluation mode, which affects the behavior of batch norm, dropout, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_errors_all = [] # these are relative error\n",
    "silent = True\n",
    "\n",
    "def vis_detections(im, class_name, dets, poses, all_poses, thresh=0.8):\n",
    "    \"\"\"Visual debugging of detections.\"\"\"\n",
    "    for i in range(np.minimum(10, dets.shape[0])):\n",
    "        bbox = tuple(int(np.round(x)) for x in dets[i, :4])\n",
    "        score = dets[i, -1]\n",
    "        if score > thresh:\n",
    "            cv2.rectangle(im, bbox[0:2], bbox[2:4], (0, 204, 0), 2)\n",
    "            cv2.putText(im, '%s: %.3f Index %d' % (class_name, score, i), (bbox[0], bbox[1] + 15), cv2.FONT_HERSHEY_PLAIN,\n",
    "                        1.0, (0, 0, 255), thickness=1)\n",
    "            # apply inverse scaling to true values\n",
    "            poses[i] = poses[i] / np.array(cfg.POSE_INV_VARS) + np.array(cfg.POSE_MEANS)\n",
    "            if not silent:\n",
    "                print class_name, i, \"pose:\"\n",
    "                print [\"{0:0.2f}\".format(p) for p in poses[i]]\n",
    "            # append pose error to pose_erros_all\n",
    "            if len(all_poses) == 0 or poses.shape[0] > len(all_poses):\n",
    "                continue\n",
    "            min_err = None\n",
    "            min_dist = 100000\n",
    "            for label in all_poses:\n",
    "                dist = 0\n",
    "                err = []\n",
    "                for k in range(len(label)):\n",
    "                    dist += abs(poses[i,k]-label[k])/abs(label[k]) # relative error\n",
    "                    err.append(abs(poses[i,k]-label[k])/abs(label[k]))\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    min_err = err\n",
    "            pose_errors_all.append(min_err)\n",
    "                    \n",
    "    return im\n",
    "\n",
    "\n",
    "def im_detect(net, image, disparity):\n",
    "    \"\"\"Detect object classes in an image given object proposals.\n",
    "    Returns:\n",
    "        scores (ndarray): R x K array of object class scores (K includes\n",
    "            background as object category 0)\n",
    "        boxes (ndarray): R x (4*K) array of predicted bounding boxes\n",
    "    \"\"\"\n",
    "    # image.shape = H x W x 3\n",
    "    # im_data, im_scales = net.get_image_blob(image)\n",
    "    im_data, im_scales, disp_data = net.frcnn.get_image_disparity_blob(image, disparity)\n",
    "    # get_image_blob() tries to make H = 600 but keep W <= 1000\n",
    "    # image gets resized, and the resize scale is returned\n",
    "    # im_data: 1 x H x W x 3\n",
    "    # im_scales: array with one number\n",
    "\n",
    "    im_info = np.array(\n",
    "        [[im_data.shape[1], im_data.shape[2], im_scales[0]]],\n",
    "        dtype=np.float32)\n",
    "\n",
    "    cls_prob, bbox_pred, pose_pred, rois = net(im_data, im_info, disp_data)\n",
    "    # cls_prob.shape = (300, 4) # KITTI has four classes, '__background__', 'Car', 'Pedestrian', 'Cyclist'\n",
    "    # bbox_pred.shape = (300, 16)\n",
    "    # rois.shape = (300, 5)\n",
    "    # pose_pred.shape = (300, 7)\n",
    "    \n",
    "    scores = cls_prob.data.cpu().numpy()\n",
    "    boxes = rois.data.cpu().numpy()[:, 1:5] / im_info[0][2]\n",
    "    poses = pose_pred.data.cpu().numpy() # convert pose prediction from torch to numpy\n",
    "    cfg.TEST.BBOX_REG = True\n",
    "    if cfg.TEST.BBOX_REG: # True in this case\n",
    "        # Apply bounding-box regression deltas\n",
    "        box_deltas = bbox_pred.data.cpu().numpy()\n",
    "        pred_boxes = bbox_transform_inv(boxes, box_deltas)\n",
    "        pred_boxes = clip_boxes(pred_boxes, image.shape)\n",
    "        # pred_boxes.shape = (300, 16)\n",
    "    else:\n",
    "        # Simply repeat the boxes, once for each class\n",
    "        pred_boxes = np.tile(boxes, (1, scores.shape[1]))\n",
    "\n",
    "    return scores, pred_boxes, poses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of images:  1480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "faster_rcnn/faster_rcnn.py:76: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  rpn_cls_prob = F.softmax(rpn_cls_score_reshape)\n",
      "faster_rcnn/pose_net.py:100: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  cls_prob = F.softmax(cls_score)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (5,) (7,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1ea6e2723d50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvis\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0mim2show\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvis_detections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim2show\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls_dets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposes_keep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_poses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mall_boxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls_dets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-0d26af0310d9>\u001b[0m in \u001b[0;36mvis_detections\u001b[0;34m(im, class_name, dets, poses, all_poses, thresh)\u001b[0m\n\u001b[1;32m     12\u001b[0m                         1.0, (0, 0, 255), thickness=1)\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m# apply inverse scaling to true values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mposes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mposes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOSE_INV_VARS\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcfg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOSE_MEANS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msilent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0;32mprint\u001b[0m \u001b[0mclass_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"pose:\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (5,) (7,) "
     ]
    }
   ],
   "source": [
    "max_per_image=300\n",
    "thresh=0.05\n",
    "# idx_image = 6 # the idx of the image that we want to test\n",
    "num_images = len(imdb.image_index)\n",
    "idx_image = np.random.randint(0, num_images, 1)[0]\n",
    "\n",
    "\n",
    "# all detections are collected into:\n",
    "#    all_boxes[cls][image] = N x 5 array of detections in\n",
    "#    (x1, y1, x2, y2, score)\n",
    "all_boxes = [[[] for _ in xrange(num_images)]\n",
    "             for _ in xrange(imdb.num_classes)]\n",
    "\n",
    "\n",
    "# timers\n",
    "_t = {'im_detect': Timer(), 'misc': Timer()}\n",
    "\n",
    "print \"# of images: \", num_images\n",
    "\n",
    "for i in range(200):\n",
    "# for i in range(num_images):\n",
    "# for i in range(idx_image,idx_image+1):\n",
    "    im_path = imdb.image_path_at(i)\n",
    "    folder = im_path[0:-10]\n",
    "    img_name = im_path[-10:-3]\n",
    "    img_name += 'png'\n",
    "    disp_path = folder + 'disparity/' + img_name # path to disparity map\n",
    "    \n",
    "    im = cv2.imread(im_path)\n",
    "    disp = cv2.imread(disp_path)\n",
    "    \n",
    "    img_index = im_path[-10:-4]\n",
    "    label_path = \"/media/data/kitti_dataset/2d_object/data_object_image_2/training/label_2/\" + img_index + \".txt\"\n",
    "    #print label_path\n",
    "    \n",
    "    # print out the groundtruth label\n",
    "    if not silent:\n",
    "        print \"\\n\"\n",
    "        print \"Groundtruth labels:\"\n",
    "        f = open(label_path, 'r')\n",
    "        print(f.read())\n",
    "        f.close()\n",
    "    # extract labels\n",
    "    f = open(label_path, 'r')\n",
    "    line = f.readline()\n",
    "    all_poses = []\n",
    "    while line:\n",
    "        info = line.split()\n",
    "        line = f.readline()\n",
    "        if info[0] not in [\"Car\", \"Cyclist\", \"Pedestrian\"]:\n",
    "            # print info[0]\n",
    "            continue\n",
    "        pose = info[-7:]\n",
    "        pose = [float(p) for p in pose]\n",
    "        all_poses.append(pose)\n",
    "    f.close()\n",
    "\n",
    "    \n",
    "    _t['im_detect'].tic()\n",
    "    \n",
    "    scores, boxes, poses = im_detect(net, im, disp_path)\n",
    "    # scores.shape = (300, 4)\n",
    "    # boxes.shape = (300, 16)\n",
    "    # poses.shape = (300, 7)\n",
    "    \n",
    "    detect_time = _t['im_detect'].toc(average=False)\n",
    "\n",
    "    \n",
    "    if vis:\n",
    "        # im2show = np.copy(im[:, :, (2, 1, 0)])\n",
    "        im2show = np.copy(im)\n",
    "\n",
    "    # skip j = 0, because it's the background class\n",
    "    for j in xrange(1, imdb.num_classes):\n",
    "        inds = np.where(scores[:, j] > thresh)[0]\n",
    "        cls_scores = scores[inds, j]\n",
    "        cls_boxes = boxes[inds, j * 4:(j + 1) * 4]\n",
    "        cls_dets = np.hstack((cls_boxes, cls_scores[:, np.newaxis])) \\\n",
    "            .astype(np.float32, copy=False)\n",
    "        keep = nms(cls_dets, cfg.TEST.NMS)\n",
    "        cls_dets = cls_dets[keep, :]\n",
    "        poses_keep = poses[keep, :]\n",
    "        \n",
    "        # print \"# of\", imdb._classes[j], \":\", len(keep) # print class name\n",
    "        \n",
    "        \n",
    "        if vis:\n",
    "            im2show = vis_detections(im2show, imdb.classes[j], cls_dets, poses_keep, all_poses)\n",
    "        all_boxes[j][i] = cls_dets\n",
    "\n",
    "    # Limit to max_per_image detections *over all classes*\n",
    "    _t['misc'].tic()\n",
    "    if max_per_image > 0:\n",
    "        image_scores = np.hstack([all_boxes[j][i][:, -1]\n",
    "                                  for j in xrange(1, imdb.num_classes)])\n",
    "        if len(image_scores) > max_per_image:\n",
    "            image_thresh = np.sort(image_scores)[-max_per_image]\n",
    "            for j in xrange(1, imdb.num_classes):\n",
    "                keep = np.where(all_boxes[j][i][:, -1] >= image_thresh)[0]\n",
    "                all_boxes[j][i] = all_boxes[j][i][keep, :]\n",
    "    nms_time = _t['misc'].toc(average=False)\n",
    "\n",
    "    print 'im_detect: {:d}/{:d} {:.3f}s {:.3f}s' \\\n",
    "        .format(i + 1, num_images, detect_time, nms_time)\n",
    "        \n",
    "\n",
    "    if vis and not silent:\n",
    "        #cv2.imshow('test', im2show)\n",
    "        #cv2.waitKey(0)\n",
    "        plt.imshow(im2show)\n",
    "        #pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pose_errors_all = np.array(pose_errors_all)\n",
    "print pose_errors_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print np.mean(pose_errors_all, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "temp",
   "language": "python",
   "name": "temp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
